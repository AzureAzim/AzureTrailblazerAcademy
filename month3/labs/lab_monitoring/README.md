# Azure Monitoring Lab

## Prerequisites

- Microsoft Azure subscription
- Resource Group to deploy Azure services
- Permissions to create VMs with public IP addresses

# Lab Setup

## Step 1: Deploy a Log Analytics Workspace

1. In the Azure Portal, search for **Log Analytics Workspaces**
2. Click on the **Add** button
3. Fill out the **Basics** tab as follows:
- **Subscription:** Choose your subscription
- **Resource Group:** Select the Resource Group you created for this lab.
- **Name:** Choose a unique name for Log Analytics Workspace. Ex: ata-la-workspace
- **Region:** East US

![VM Basics Tab](images/la_basics.png)

4. Click the **Review + Create** button
5. Click the **Create** button


## Step 2: Deploy a Virtual Machine

1. In the Azure Portal, search for **Virtual Machines**
2. Click on the **Add** button
3. Fill out the **Basics** tab as follows:
- **Subscription:** Choose your subscription
- **Resource Group:** Select the Resource Group you created for this lab.
- **Virtual Machine Name:** Choose a unique name for the VM. Ex: ata-vm1
- **Region:** East US
- **Availability Options:** No infrastructure redundancy required
- **Image:** Windows Server 2019 Datacenter
- **Azure Spot Instance:** No
- **Size:** Standard D2s v3
- **Authentication Type:** Password
- **Username:** Enter your user name. Ex: ata-user
- **Password:** Enter your password
- **Select inbound ports:** RDP(3389)

![VM Basics Tab](images/vm_basics.png)

4. Click the **Next: Disks** button
5. Leave everything as default and click the **Next: Networking** button
6. Fill out **Networking** tab as follows:
- **Virtual Network:** Leave as default. If no other VNets exist in the RG, it should create a new VNet for the VM.
- **Subnet:** Leave as default
- **Public IP:** Leave as default to create a new public IP
- Leave everything else as default

![VM Networking Tab](images/vm_networking.png)

7. Click the **Next: Management** button
8. Inside the **Management** tab:
- Turn off **Boot diagnostics** and **Auto-Shutdown**
- Leave everything else as default

9. Click the **Next: Advanced** button
10. Leave everything as default and click on the **Next: Tags** button
11. Enter any custom tags (optional) and click the **Next: Review + Create** button
12. Click the **Create** button

## Step 3: Enable VM Insights

Follow these steps to enable VM Insights for the VM. Since it takes a couple of minutes for VM Insights to be enabled, you can complete steps below and then move to the next section of the lab. You don't need to wait for that to finish to complete the next section **Metrics Monitoring**.

1. Open the VM resource once it is provisioned
2. On the left blade, select **Insights**
3. Click on the **Enable** button
4. Select your subscription and the Log Analytics Workspace that you deployed in Step 1.
5. Click the **Enable** button
6. It will take a couple of minutes for VM Insights to be enabled. As part of this process, two agents will be installed in the VM: the Microsoft Monitoring Agent and the Dependency Agent. The VM will also be configured to send logs and other performance data to the Log Analytics workspace you selected.

![Enable VM Insights](images/vm_insights_enable.png)


## Step4: Metrics Monitoring

1. On the left blade click on **Metrics**
2. On the top filter select the following:
- **Scope:** Leave as default. The VM name should be there.
- **Metric Namespace:** Virtual Machine Host
- **Metric:** Disk Read Operations/Sec
- **Aggregation:** Avg

![Metrics Filter](images/vm_metrics_filter.png)

3. On the **Time Range** (top right of the screen) and select the following:
- **Time Range:** Last 30 minutes
- **Time Granularity:** 1 minute
4. Click **Apply**. The chart should have been updated now to show the CPU percentage for the last 30 minutes.

![Metrics Time Range](images/vm_metrics_timerange.png)

5. You can add multiple metrics to the same chart so let's add a second one.
6. Click on **Add metric**
7. On the new filter that was created, select the following:
- **Scope:** Leave as default. The VM name should be there.
- **Metric Namespace:** Virtual Machine Host
- **Metric:** Disk Write Operations/Sec
- **Aggregation:** Avg
8. Click anywhere on the screen and you should now see two lines charts inside the same chart
9. Customiza the apperance of the chart by click on the **Line Chart** button (at the top) and changing it to an **Area Chart**

![Metrics Chart](images/vm_metrics_chart.png)

10. Click on the **Pin to dashboard** button and select **Pin to current dashboard**
11. In the left-side menu of the Azure Portal, click on **Dashboard** and to see the chart you just pinned there 



## Step 5: Log Monitoring and Advanced Guest OS Monitoring

If you need to analyse logs generated by your applications/Guest OS or monitor other performance metrics that are available only to the Guest OS(such as Memory available) you need to use Log Analytics. When you turned on **VM Insights** in Step 3, the VM was configured to send this logs/data to the Log Analytics workspace where we can query it and analyize it. Let's take a look at the data that has been collected.

1. Go back to your resource group and open the Virtual Machine
2. On the left menu, click on **Logs**. This will open the workspace that we can use to query the logs that have been collected and stored in the Log Analytics workspace created in Step 1.
3. On the left side of the workspace, you should see a list of tables that can be queried such as Perf, Heartbeat, Syslog, etc.
4. On the right side, type the following to query all records from the **Heartbeat** table:

    ```
    Heartbeat
    ```
5. Click the **Run** button and you should see a list of of everytime this VM has sent a "keep-alive" or "heartbeat" message the Log Analytics Workspace in the last 24 hour.

![Log Analytics Workspace](images/vm_laworkspace.png)

6. Let's now explore the records for the "InsightsMetrics" table. That table contains useful metrics that are sent directly from the Guest Operating System. Type the following and then click **Run**:

    ```
    InsightMetrics
    ```
7. You will see many rows of data coming from different performance counters such as Processor, Memory and LogicalDisk. Let's execute the following query to filter the table and get metrics for "Available Disk Space" only:

    ```
    InsightsMetrics| where Name == "FreeSpaceMB" and Namespace == "LogicalDisk"| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 1min)
    ```

8. Sometime it is easier (and/or makes more sense) to look at a chart instead of rows of data. Let's execute the following query to create a chart of the "Available Memory" for this VM. Notice the "render timechart" at the end of the query:

    ```
    InsightsMetrics| where Name == "AvailableMB" and Namespace == "Memory"| summarize AggregatedValue = avg(Val) by bin(TimeGenerated, 1min), Computer| render timechart
    ```


![Log Analytics Memory Chart](images/vm_loganalytics_chart.png)







## Step 6: Connect other Azure Services


## Step 7: Create an alert





## Step 3: Enable VM Insights and Create Metrics Charts

Follow the following steps to enable VM Insights in the VM. It will take a couple of minutes. Enabling VM Insights will deploy two agents inside the VM: The Microsoft Monitoring Agent and the Dependency Agent.

1. 

Press the "*Deploy to Azure*" button below, to provision the Azure Services required required for this lab.

<a href="https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fmicrosoft%2FAzureTrailblazerAcademy%2Fmaster%2Fmonth2%2Flabs%2Flab_data%2Fscripts%2Flab2_data_deployment.json" target="_blank"><img src="https://azuredeploy.net/deploybutton.png"/></a>

## Step 2: Deploy Azure Data Factory

While those other resources are being deployed, follow these steps to manually deploy Azure Data Factory.

1. In the Azure Portal, search for **Data Factories**
2. Click on the **Add** button
3. Fill out the form:
- **Name:** Choose a unique name for your Data Factory
- **Version:** V2
- **Subscription:** Choose your subscription
- **Resource Group:** Use the same Resource Group you used for the automated deployment in Step 1.
- **Location:** East US
- Uncheck the **Enable GIT** checkbox
4. Click the **Create** button


## Step 3: Validation of services

Estimated Time: 15 minutes
 
1. Once the automated deployment is complete, expand the resource group you used previosly and review that the following services are provisioned:

- Azure Synapse Analytics (SQL Data Warehouse)
- Azure Storage Accounts
- Azure Data Factory

2. Capture the name of your storage acccount (Data Lake Storage)
3. CLick on the SQL Data Warehouse
4. Click on **Firewalls and virtual networks** and select **Add Client IP**. Then click on **Save**.

![Adding Client IP to SQL Server Firewall](images/Firewall.png)

5. Click on **Query Editor** and login using the user name and password you entered earlier.
6. Execute the following query:

```
CREATE MASTER KEY;
```

## Step 4: Populate the Azure Storage container Azure CloudShell

1. In the Azure Portal, open an Azure Cloudshell window.
   Complete the configuration process as needed

2. Type the following command to copy the deployment script to Azure Cloud Shell

   This will copy the deployment script that will populate the Azure Data Lake Storage Containers with sample files

```
curl -OL https://raw.githubusercontent.com/microsoft/AzureTrailblazerAcademy/master/month2/labs/lab_data/scripts/data_script.sh
```
1. Type the following to upload the sample files:

```
bash data_script.sh <storageaccountname>
```
Example: bash data_script.sh mdwdemostoredl

3. Click on the storage account and browse to **Containers**:

    - Validate that 2 containers **data** and **logs** are existing
    - Click on the **data** container and validate that these 2 files exist:
        - preferences.json
        - DimDate2.txt
    - Repeat the validation for the **logs** container as well, validate that these 3 files exist in the container:
        - preferences.json
        - weblogsQ1.log
        - weblogsQ2.log
## Step 5: Ingest data using the Copy Activity
  
Estimated Time: 15 minutes

### Task 1: Add the Copy Activity to the designer

1. Navigate to the resource group and open the Azure Data Factory resource.

1. In the Azure Data Factory screen, in the middle of the screen, click on the button, **Author & Monitor**

1. **Open the authoring canvas** If coming from the ADF homepage, click on the **pencil icon** on the left sidebar or the **create pipeline button** to open the authoring canvas.

1. **Create the pipeline** Click on the **+** button in the Factory Resources pane and select **Pipeline**.

1. Search for **Copy data** In the Activities pane and drag the accordion onto the pipeline canvas.

    ![Adding the Copy Activity to Azure Data Factory in the Azure Portal](images/M07-E02-T01-img01.png)


### Task 2: Create a new HTTP dataset to use as a source

1. Click on the **Copy data** activity and, in the Source tab of the activity settings, click **+ New**

2. In the data store list, select the **HTTP** tile and click continue

3. In the file format list, select the **DelimitedText** format tile and click continue

4. In Set Properties blade, give your dataset an understandable name such as **HTTPSource**. Click on the **Linked Service** dropdown and select **New**.

5. In the New Linked Service (HTTP) screen, specify the url of the moviesDB csv file. You can access the data with no authentication required using the following endpoint:

    https://raw.githubusercontent.com/microsoft/AzureTrailblazerAcademy/master/month2/labs/lab_data/data/moviesDB.csv

6. Place this in the **Base URL** text box. 

7. In the **Authentication type** drop down, select **Anonymous**. and click on **Create**.


8. Once you have created and selected the linked service, specify the rest of your dataset settings. These settings specify how and where in your connection we want to pull the data. As the url is pointed at the file already, no **Relative URL** is required. As the data has a header in the first row, set **First row as header** to be true and select Import schema **From connection/store** to pull the schema from the file itself. In the **Request method** dropdown, select **Get**. 

    You should see the following screen:

    ![Creating a linked service and dataset in Azure Data Factory in the Azure Portal](images/M07-E02-T02-img01.png)
           
9. Click **OK** once completed.
   
10. To verify your dataset is configured correctly, click **Preview Data** in the Source tab of the copy activity to get a small snapshot of your data.
   
   ![Previewing in Azure Data Factory in the Azure Portal](images/M07-E02-T02-img02.png)

### Task 3: Create a new ADLS Gen2 dataset sink

1. Click on the **Sink tab**, and the click **+ New**

2. Select the **Azure Data Lake Storage Gen2** tile and click **Continue**.

3. Select the **DelimitedText** format tile and click **Continue**.

4. In Set Properties blade, give your dataset an understandable name such as **ADLSG2**. Click on the **Linked Service** dropdown and select **New**.

5. In the New linked service (Azure Data Lake Storage Gen2) blade, select your authentication method as **Account key**, select your **Azure Subscription** and select your Storage account name of **awdlsstudxx**. You will see a screen as follows:

   ![Create a Sink in Azure Data Factory in the Azure Portal](images/M07-E02-T03-img01.png)

6. Click on **Create**

7. Once you have configured your linked service, you enter the set properties blade. As you are writing to this dataset, you want to point the folder where you want moviesDB.csv copied to. In the example below, you are writing to the  **data** folder. While the folder can be dynamically created, the file system must exist prior to writing to it.
    * Set **File path** to the 'data' folder in ADLS
    * Set **First row as header** to be true. 
    * Set **Import schema** to **From connection/store**


![Setting properties of a Sink in Azure Data Factory in the Azure Portal](images/M07-E02-T03-img02.png)

8. Click **OK** once completed.

### Task 4: Test the Copy Activity

At this point, you have fully configured your copy activity. To test it out, click on the **Debug** button at the top of the pipeline canvas. This will start a pipeline debug run.

1. To monitor the progress of a pipeline debug run, click on the **Output** tab of the pipeline

2. To view a more detailed description of the activity output, click on the eyeglasses icon. This will open up the copy monitoring screen which provides useful metrics such as Data read/written, throughput and in-depth duration statistics.

   ![Monitoring a pipeline in Azure Data Factory in the Azure Portal](images/M07-E02-T04-img01.png)

3. To verify the copy worked as expected, open up your ADLS gen2 storage account and check to see your file was written as expected

4. Click **Publish All** to deploy the pipeline to the factory

## Step 6: Preparing Data Factory Environment

### Task 1: Create Data Flow and add Data Source

1. **Turn on Data Flow Debug** Turn the **Data Flow Debug** slider located at the top of the authoring module on. 

    > NOTE: Data Flow clusters take 5-7 minutes to warm up.

2. Search for **Data Flow** In the Activities pane and drag the accordion onto the pipeline canvas. In the blade that pops up, click **Create new Data Flow** and select **Mapping Data Flow** and then click **OK**.

3. Click on the  **pipeline1** tab and drag the green box from your Copy activity to the Data Flow Activity to create an on success condition. You will see the following in the canvas:

    ![Adding a Mapping Data Flow in Azure Data Factory](images/M07-E03-T01-img01.png)

4. Double click on the Mapping Data Flow object in the canvas and click on the **Add Source** button in the Data Flow canvas. In the **Source settings** tab click on the **Source dataset** dropdown and select the **ADLSG2** dataset that you used in your Copy activity.

    ![Adding a Source to a Mapping Data Flow in Azure Data Factory](images/M07-E03-T02-img01.png)

5. Click on the **Projection** tab
6. Click on **Import Schema** and wait for source schema to be displayed
7. Once your debug cluster is warmed up, verify your data is loaded correctly via the Data Preview tab. Once you click the refresh button, Mapping Data Flow will show calculate a snapshot of what your data looks like when it is at each transformation.
  
### Task 2: Using Mapping Data Flow transformation

1. In the preview of the data, you may have noticed that the "Rotton Tomatoes" column is misspelled. To correctly name it and drop the unused Rating column, you can add a [Select transformation](https://docs.microsoft.com/azure/data-factory/data-flow-select) by clicking on the + icon next to your ADLS source node and choosing **Select** under the **Schema modifier** section.
    
    ![Adding a Transformation to a Mapping Data Flow in Azure Data Factory](images/M07-E03-T03-img01.png)

2. In the **Name as** field, change 'Rotton' to 'Rotten'. Drop the Rating column by hovering over it and clicking on the trash can icon.

    ![Using the Select Transformation to a Mapping Data Flow in Azure Data Factory](images/M07-E03-T03-img02.png)

3. Since we are only interested in movies made after 1951, we'll add a [Filter transformation](https://docs.microsoft.com/azure/data-factory/data-flow-filter) to specify a filter condition. Click on the **+ icon** next to your Select transformation and choose **Filter** under the **Row Modifier** section.

4. Click on the **expression box** to open up the [Expression builder](https://docs.microsoft.com/azure/data-factory/concepts-data-flow-expression-builder) and enter in your filter condition. Using the syntax of the [Mapping Data Flow expression language](https://docs.microsoft.com/azure/data-factory/data-flow-expression-functions), enter the following:

    ```
    toInteger(year) > 1950
    ```


    ![Using the Expression Builder in the Mapping Data Flow in Azure Data Factory](images/M07-E03-T03-img04.png)

5. You can click the **Refresh** button to verify your condition is working properly in the embedded Data preview pane. Click **Save and finish**.

6. As you may have noticed, the genres column is a string delimited by a '|' character. We only care about the *first* genre in each column so we are going to derive a new column called **PrimaryGenre** using the [Derived Column](https://docs.microsoft.com/azure/data-factory/data-flow-derived-column) transformation. Click on the **+ icon** next to your Filter transformation and choose **Derived** under the **Schema Modifier** section. Similar to the filter transformation, the derived column uses the Mapping Data Flow expression builder to specify the values of the new column.

    ![Using the Derived Transformation to a Mapping Data Flow in Azure Data Factory](images/M07-E03-T03-img05.png)

7. Create a new column called PrimaryGenre and enter the following in the expression field:

    ```
    iif(locate('|',genres)>1,left(genres,locate('|',genres)-1),genres)
    ```

8. Click **Save and finish**.
   
9. We are interested in how a movie ranks within its year for its specific genre. For that, we will add a [Window transformation](https://docs.microsoft.com/azure/data-factory/data-flow-window) to define window-based aggregations. Click on the **+ icon** next to your Derived Column transformation and select **Window** under the **Schema modifier** section. We will window over PrimaryGenre and year with an unbounded range, sort by Rotten Tomato descending, a calculate a new column called RatingsRank which is equal to the rank each movie has within its specific genre-year. See below:

    ![Window Over](images/WindowOver.PNG "Window Over")

    ![Window Sort](images/WindowSort.PNG "Window Sort")

    ![Window Bound](images/WindowBound.PNG "Window Bound")

    ![Window Rank](images/WindowRank.PNG "Window Rank")

10. Now that you have gathered and derived all your required data, we will add an [Aggregate transformation](https://docs.microsoft.com/azure/data-factory/data-flow-aggregate) to calculate metrics based on a desired group. Click on the **+ icon** next to your Window transformation and select **Aggregate** under the **Schema modifier** section. As you did in the window transformation, lets group movies by PrimaryGenre and year

    ![Using the Aggregate Transformation to a Mapping Data Flow in Azure Data Factory](images/M07-E03-T03-img10.png)

11. In the Aggregates tab, you can aggregations calculated over the specified group by columns. For every genre and year (selected in the "Group By" tab), lets get the average Rotten Tomatoes rating, the highest and lowest rated movie (utilizing the windowing function) and the number of movies that are in each group. Aggregation significantly reduces the amount of rows in your transformation stream and only propagates the group by and aggregate columns specified in the transformation.

    ![Configuring the Aggregate Transformation to a Mapping Data Flow in Azure Data Factory](images/M07-E03-T03-img11.png)

    These are the 4 expressions shown in the previous screenshot. You can use those to copy and paste in the **Aggregates** section above:

    ```
    avg(toInteger({Rotten Tomato}))
    ```
    ```
    first(title)
    ```
    ```
    last(title)
    ```
    ```
    count()
    ```

12. Go to **Data Preview** tab and click on **Refresh** to see how the aggregate transformation changes your data.
   

13. Since we are writing to a tabular sink, we can specify insert, delete, update and upsert policies on rows using the [Alter Row transformation](https://docs.microsoft.com/azure/data-factory/data-flow-alter-row). Click on the **+ icon** next to your Aggregate transformation and click **Alter Row** under the **Row modifier** section. Since we are always inserting and updating, you can specify that all rows will always be upserted by entering **true()** in the expression field.

    ![Using the Alter Row Transformation to a Mapping Data Flow in Azure Data Factory](images/M07-E03-T03-img12.png)
    

### Task 3: Writing to a Data Sink

Now that you have finished all your transformation logic, you are ready to write to a Sink.

1. Add a **Sink** by clicking on the **+ icon** next to your Upsert transformation and clicking **Sink** under the **Destination** section.

2. In the Sink tab, create a new data warehouse dataset via the **+ New** button.

3. Select **Azure Synapse Analytics** from the tile list and click **Continue**.

4. Open the *Linked service** dropdown and click on **+ New** to configure a Azure Synapse Analytics connection to connect to the Data Warehouse that we created earlier. Select the datawarehouse from the dropdown, select **SQL authentication** as the **Authentication type** and the **user name** and **password** you used when you created the Datawarehouse.

    ![Creating an Azure Synapse Analytics connection in Azure Data Factory](images/M07-E03-T04-img01.png)

5. Click **Create**.

6. In the dataset configuration, select **Create new table** and enter in **Dbo** in the Schema field and **Ratings** in table name field. Click **OK** once completed.

    ![Creating an Azure Synapse Analytics table in Azure Data Factory](images/M07-E03-T04-img02.png)

7. Since an upsert condition was specified, you need to go to the Settings tab and select 'Allow upsert' based on key columns PrimaryGenre and year.

    ![Configuring Sink settings in Azure Data Factory](images/M07-E03-T04-img03.png)

At this point, You have finished building your 8 transformation Mapping Data Flow. It's time to run the pipeline and see the results!

![Completed Mapping Data Flow in Azure Data Factory](images/M07-E03-T04-img04.png)

## Task 4: Running the Pipeline

1. Go to the pipeline1 tab in the canvas. Since Azure Synapse Analytics in Data Flow uses [PolyBase](https://docs.microsoft.com/sql/relational-databases/polybase/polybase-guide?view=sql-server-2017), you must specify a blob or ADLS staging folder. In the Data Flow activity's settings tab, open up the PolyBase section and select your ADLS linked service and specify **data** as the container and **dw-staging** as the staging directory.

    ![PolyBase configuration in Azure Data Factory](images/M07-E03-T05-img01.png)

2. Before you publish your pipeline, run another debug run to confirm it's working as expected. Looking at the Output tab, you can monitor the status of both activities as they are running.

3. Once both activities succeeded, you can click on the eyeglasses icon next to the Data Flow activity to get a more in depth look at the Data Flow run.

4. If you used the same logic described in this lab, your Data Flow should will written 737 rows to your SQL DW.

5. To verify that, go back to the Azure Portal and expand the data warehouse blade in your resource group. Click on **Query Editor (preview)** from the SQL data warehouse blade and write the following query:

    ```
    select count(*) from dbo.Ratings
    ```

6. Finally, to see the data that was loaded in the Data Warehouse, enter the following query:

    ```
    select * from dbo.Ratings
    ```

7. If you are happy with the results, you can go back to Data Factory and click on the **Publish All** button at the top to save all changes to your pipeline.